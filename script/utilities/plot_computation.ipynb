{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Computations on IMODAL\n\nIn this tutorial we show how to perform computations on GPU with KeOps.\n\nKeOps, which stands for KErnelOPerationS, allow us to perform kernel matrix reductions.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import relevant Python modules.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\nsys.path.append(\"../\")\nimport time\n\nimport torch\n\nimport imodal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each deformation modules that needs kernel matrix reduction has a plain Pytorch and KeOps implementation.\n\nIn order to select once and for all the computation backend one can simply call\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "imodal.Utilities.set_compute_backend('keops')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Available compute backend are **keops** and **torch**. All subsequent deformation modules that are created will use the specified computation backend.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "d = 2\nN = 10000\nsigma = 1e-3\n\ntranslations = imodal.DeformationModules.Translations(d, N, sigma)\n\nprint(translations.backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Changing computation backend will not affect already created modules and these will have to be initialized again.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible to explicitely set the compute backend for a deformation module using the **backend** keyword.\nThe varifold attachment also offers a KeOps implementation in 3D (but not in 2D yet).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "keops_translations = imodal.DeformationModules.Translations(d, N, sigma, backend='keops')\n\ntorch_translations = imodal.DeformationModules.Translations(d, N, sigma, backend='torch')\n\nattachment = imodal.Attachment.VarifoldAttachment(3, [1.], backend='keops')\n\nprint(keops_translations.backend)\nprint(torch_translations.backend)\nprint(attachment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moving computation on GPUs can be done. As IMODAL is built on top of Pytorch, the device parameter can either be a string or a torch.device object.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "positions = torch.randn(N, d)\nkeops_translations.manifold.fill_gd(positions)\nkeops_translations.to_(device='cuda')\n\ntorch_translations = imodal.DeformationModules.Translations(d, N, sigma, positions.to(device='cuda'), backend='torch')\n\nprint(keops_translations.device)\nprint(torch_translations.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>It is thus possible to select the GPU on which to perform computations by putting **device='cuda:x'** where **x** specify the GPU index (such as given by the **nvidia-smi** command.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>For now, when choosing the second method to select the computing device, susequent manifold filling with a different device will lead in an degenerate states which will ultimately fail. It is thus best to use the **to_()** method.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p></p></div>\n The **to_()** method can also be used to change the dtype of the module tensors, in exacly the same way as with Pytorch.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compare performance for \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "points = torch.randn(N, d, device='cuda')\n\nstart = time.perf_counter()\nkeops_translations(points)\nprint(\"KeOps backend on GPU, elapsed timed={}\".format(time.perf_counter() - start))\n\nstart = time.perf_counter()\ntorch_translations(points)\nprint(\"Pytorch backend on GPU, elapsed timed={}\".format(time.perf_counter() - start))\n\nimodal.Utilities.set_compute_backend('torch')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}